{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "def getSource(url):\n",
    "    response = urlopen(url)\n",
    "    html = response.read()\n",
    "    return html.decode()\n",
    "\n",
    "def news(url, wordlist):\n",
    "    \n",
    "    #decode web response\n",
    "    news = getSource(url)\n",
    "    \n",
    "    #count words\n",
    "    for word in wordlist:\n",
    "        occurences = news.count(word)\n",
    "        print('{} appears {} times'.format(word, occurences))\n",
    "\n",
    "news('https://bbc.co.uk', ['economy', 'climate', 'education'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser\n",
    "\n",
    "class LinkParser(HTMLParser):\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        \n",
    "        if tag == 'a':\n",
    "            for attr in attrs:\n",
    "                if attr[0] == 'href':\n",
    "                    print(attr[1])\n",
    "\n",
    "infile = open('links.html')\n",
    "content = infile.read()\n",
    "infile.close()\n",
    "\n",
    "parser = LinkParser()\n",
    "parser.feed(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser\n",
    "\n",
    "class MyHTMLParser(HTMLParser):\n",
    "    \n",
    "    indent = 0\n",
    "    \n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        print('{}{} start'.format(self.indent*' ', tag))\n",
    "        self.indent += 4\n",
    "        \n",
    "    def handle_endtag(self, tag):\n",
    "        self.indent -= 4\n",
    "        print('{}{} end'.format(self.indent*' ', tag))\n",
    "        \n",
    "infile = open('w3c.html')\n",
    "content = infile.read()\n",
    "infile.close()\n",
    "\n",
    "myParser = MyHTMLParser()\n",
    "myParser.feed(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urljoin\n",
    "from html.parser import HTMLParser\n",
    "\n",
    "class Collector(HTMLParser):\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        HTMLParser.__init__(self)\n",
    "        self.url = url\n",
    "        self.links = []\n",
    "        self.text = []\n",
    "        \n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag == 'a':\n",
    "            for attr in attrs:\n",
    "                if attr[0] == 'href':\n",
    "                    absolute = urljoin(self.url, attr[1])\n",
    "                    if absolute[:5] == 'https':\n",
    "                        self.links.append(absolute)\n",
    "    \n",
    "    def handle_data(self, data):\n",
    "        self.text.append(data)\n",
    "    \n",
    "    def getData(self):\n",
    "        return self.text\n",
    "    \n",
    "    def getLinks(self):\n",
    "        return self.links\n",
    "\n",
    "url = 'https://www.w3.org/Consortium/mission.html'\n",
    "resource = urlopen(url)\n",
    "content = resource.read().decode()\n",
    "collector = Collector(url)\n",
    "collector.feed(content)\n",
    "for link in collector.getLinks():\n",
    "    print(link)\n",
    "for txt in collector.getData():\n",
    "    print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import findall\n",
    "\n",
    "def frequency(content):\n",
    "    words = findall('[a-zA-Z]+', content)\n",
    "    \n",
    "    dictionary = {}\n",
    "    \n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            dictionary[word] += 1\n",
    "        else:\n",
    "            dictionary[word] = 1\n",
    "    \n",
    "    return dictionary\n",
    "\n",
    "cc = 'The pure and simple trush is rarely pure and never\\ simple'\n",
    "frequency(cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import findall\n",
    "\n",
    "infile = open('frankenstein.txt', 'r')\n",
    "content = infile.read()\n",
    "infile.close()\n",
    "\n",
    "# print(findall('Frankenstein', content))\n",
    "# print(findall('[\\d]+', content))\n",
    "# print(findall('[\\w]+ible', content))\n",
    "# print(findall('[A-Z][\\w]*y', content))\n",
    "# print(findall('horror of [a-z][a-z]?', content))\n",
    "# print(findall('[\\w]+[\\s]death', content))\n",
    "# print(findall('[\\w\\s]*laboratory[\\w]*[\\s]*\\.', content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import findall\n",
    "\n",
    "infile = open('links.html', 'r')\n",
    "content = infile.read()\n",
    "infile.close()\n",
    "\n",
    "print(findall('href=\"[\\S]*\"', content))\n",
    "\n",
    "USD = '$[1-9,]?[\\d]?[\\d]?[\\d]?\\.[\\d\\d]?'\n",
    "date = '[0-3][\\d]/[0-1][\\d]/[\\d\\d\\d\\d]'\n",
    "email = '[\\w]*@[\\w]*\\.[\\w]*'\n",
    "url = 'http://[\\w]*\\.?[\\w]*\\.[\\w]+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser\n",
    "\n",
    "class UlOl(HTMLParser):\n",
    "    \n",
    "    ul = []\n",
    "    indent = 0\n",
    "    val = 0\n",
    "    \n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag == 'ul':\n",
    "            print('<{}>'.format(tag))\n",
    "            \n",
    "            \n",
    "        elif tag == 'li':\n",
    "            self.indent += 4\n",
    "            self.li.append('{}'.format(' '*self.indent))\n",
    "            self.li.append('<{}>'.format(tag))\n",
    "            self.val += 1\n",
    "    \n",
    "    def handle_endtag(self, tag):\n",
    "        if tag == 'ul':\n",
    "            print('<\\{}>'.format(tag))\n",
    "        elif tag == 'li':\n",
    "            self.indent -= 4\n",
    "            self.li.append('<\\{}>'.format(tag))\n",
    "            print(''.join(self.li))\n",
    "            self.li.clear()\n",
    "            self.val -= 1\n",
    "    \n",
    "    def handle_data(self, data):\n",
    "        if self.val == 1:\n",
    "            self.li.append(data)\n",
    "\n",
    "infile = open('w3c.html', 'r')\n",
    "content = infile.read()\n",
    "infile.close()\n",
    "\n",
    "par = UlOl()\n",
    "par.feed(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser\n",
    "\n",
    "class ListCollector(HTMLParser):\n",
    "    \n",
    "    listOfList = []\n",
    "    listContainer = []\n",
    "    val = 0\n",
    "    \n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag == 'li':\n",
    "            self.val += 1\n",
    "    \n",
    "    def handle_endtag(self, tag):\n",
    "        if tag == 'ul' or tag == 'ol':\n",
    "            listOfList.append(listContainer)\n",
    "            listContainer.clear()\n",
    "            \n",
    "        elif tag == 'li':\n",
    "            self.val -= 1\n",
    "    \n",
    "    def handle_data(self, data):\n",
    "        if self.val == 1:\n",
    "            self.listContainer.append(data)\n",
    "\n",
    "    def getLists(self):\n",
    "        return self.listOfList\n",
    "    \n",
    "infile = open('w3c.html', 'r')\n",
    "content = infile.read()\n",
    "infile.close()\n",
    "\n",
    "par = UlOl()\n",
    "par.feed(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import findall\n",
    "\n",
    "def scary(filename):\n",
    "    \n",
    "    infile = open(filename, 'r')\n",
    "    content = infile.read()\n",
    "    infile.close()\n",
    "    \n",
    "    wordList = []\n",
    "    \n",
    "    words = findall('[\\w]+', content.lower())\n",
    "    \n",
    "    for word in words:\n",
    "        if word not in wordList:\n",
    "            wordList.append(word)\n",
    "    \n",
    "    wordList.sort()\n",
    "    \n",
    "    outfile = open('dictionary.txt', 'w')\n",
    "    \n",
    "    for word in wordList:\n",
    "        print(word)\n",
    "        outfile.write(word+'\\n')\n",
    "    \n",
    "    outfile.close()\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "scary('frankenstein.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from html.parser import HTMLParser\n",
    "\n",
    "class ContentParser(HTMLParser):\n",
    "    val = 0\n",
    "    \n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag in ['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'a']:\n",
    "            self.val += 1\n",
    "    \n",
    "    def handle_endtag(self, tag):\n",
    "        if tag in ['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'a']:\n",
    "            self.val -= 1\n",
    "    \n",
    "    def handle_data(self, data):\n",
    "        if self.val == 1 and data != '' != '\\n':\n",
    "            print(data)\n",
    "\n",
    "def getSource(url):\n",
    "    response = urlopen(url)\n",
    "    html = response.read()\n",
    "    return html.decode()\n",
    "\n",
    "def getContent(url):\n",
    "    content = getSource(url)\n",
    "    parser = ContentParser()\n",
    "    parser.feed(content)\n",
    "\n",
    "getContent('https://www.nytimes.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from re import findall\n",
    "\n",
    "def emails(url):\n",
    "    content = urlopen(url).read().decode()\n",
    "    return findall('[\\w]+@[\\w]*\\.?[\\w]*\\.?[\\w]*', content)\n",
    "\n",
    "urll = 'http://www.cdm.depaul.edu'\n",
    "emails(urll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('google.html', <http.client.HTTPMessage at 0x7fae743ece50>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "\n",
    "urlretrieve('https://google.com', 'google.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
