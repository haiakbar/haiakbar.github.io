{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from html.parser import HTMLParser\n",
    "\n",
    "class Collector(HTMLParser):\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        HTMLParser.__init__(self)\n",
    "        self.url = url\n",
    "        self.links = []\n",
    "        self.text = []\n",
    "        \n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag == 'a':\n",
    "            for attr in attrs:\n",
    "                if attr[0] == 'href':\n",
    "                    absolute = urljoin(self.url, attr[1])\n",
    "                    if absolute[:5] == 'https':\n",
    "                        self.links.append(absolute)\n",
    "    \n",
    "    def handle_data(self, data):\n",
    "        self.text.append(data)\n",
    "    \n",
    "    def getData(self):\n",
    "        return self.text\n",
    "    \n",
    "    def getLinks(self):\n",
    "        return self.links\n",
    "\n",
    "def analyze(url):\n",
    "    print('Visiting', url)\n",
    "    \n",
    "    content = urlopen(url).read().decode()\n",
    "    collector = Collector(url)\n",
    "    collector.feed(content)\n",
    "    urls = collector.getlinks()\n",
    "    \n",
    "    return urls    \n",
    "    \n",
    "def crawl1(url, n):\n",
    "    links = analyze(url)\n",
    "    if n != 0:\n",
    "        for link in links:\n",
    "            try:\n",
    "                crawl1(link, n-1)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "visited = set()\n",
    "        \n",
    "def crawl2(url):\n",
    "    global visited\n",
    "    \n",
    "    visited.add(url)\n",
    "    \n",
    "    links = analyze(url)\n",
    "    \n",
    "    for link in links:\n",
    "        if link not in visited:\n",
    "            try:\n",
    "                crawl2(link)\n",
    "            except:\n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visiting one.html\n",
      "Visiting two.html\n",
      "Visiting four.html\n",
      "Visiting five.html\n",
      "Visiting three.html\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from html.parser import HTMLParser\n",
    "\n",
    "class Collector(HTMLParser):\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        HTMLParser.__init__(self)\n",
    "        self.url = url\n",
    "        self.links = []\n",
    "        self.text = []\n",
    "        \n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag == 'a':\n",
    "            for attr in attrs:\n",
    "                if attr[0] == 'href':\n",
    "                    self.links.append(attr[1])\n",
    "#                     absolute = urljoin(self.url, attr[1])\n",
    "#                     if absolute[:5] == 'https':\n",
    "#                         self.links.append(absolute)\n",
    "    \n",
    "    def handle_data(self, data):\n",
    "        self.text.append(data)\n",
    "    \n",
    "    def getData(self):\n",
    "        return self.text\n",
    "    \n",
    "    def getLinks(self):\n",
    "        return self.links\n",
    "\n",
    "class Crawler2:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.visited = set()\n",
    "        \n",
    "    def analyze(self, url):\n",
    "        print('Visiting', url)\n",
    "\n",
    "#         self.content = urlopen(url).read().decode()\n",
    "        self.content = open(url, 'r').read()\n",
    "        self.collector.feed(self.content)\n",
    "        return self.collector.getLinks()\n",
    "    \n",
    "    def crawl2(self, url):\n",
    "        \n",
    "        self.collector = Collector(url)\n",
    "        \n",
    "        self.visited.add(url)\n",
    "    \n",
    "        links = self.analyze(url)\n",
    "\n",
    "        for link in links:\n",
    "            if link not in self.visited:\n",
    "                try:\n",
    "                    self.crawl2(link)\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "yo = Crawler2()\n",
    "yo.crawl2('one.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(url):\n",
    "    \n",
    "    print('Visiting', url)\n",
    "    \n",
    "    content = urlopen(url).read().decode()\n",
    "    collector = Collector(url)\n",
    "    collector.feed(content)\n",
    "    urls = collector.getLinks()\n",
    "    \n",
    "    content = collector.getData()\n",
    "    freq = frequency(content)\n",
    "    \n",
    "    print('\\n{:50} {:10} {:5}'.format('URL', 'word', 'count'))\n",
    "    for word in freq:\n",
    "        print('{:50} {:10} {:5}.format(url, word, freq[word])')\n",
    "        \n",
    "    print('\\n{:50} {:10}'.format('URL', 'link'))\n",
    "    for link in urls:\n",
    "        print('{:50} {:10}'.format(url, link))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
